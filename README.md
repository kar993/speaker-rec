speakerRec.py: a vggish model is used to implement a speaker recognition system. the training dataset is two audio files, each about 4 mins duration and one is female voice and the other is a male voice. the testing/validation dataset is the audio sample eaach of about 1 min from the same people. the words and sentences in training and testing dataset are not similar. The accuracy of this model is around 52%.
audio_recognition.py: a cnn model is used to implement this. same dataset as earlier is used here. In this, the audio clips are divided into many chunks of 1s duration. Then mel-spectrograms are taken from those smaller clips. The mel-spectrogram images are used to train a CNN model. This model is giving a 100% validation accuracy.
